{"pageProps":{"frontmatter":{"slug":"ml-candle","title":"Machine Learning Predicts Candle Burn Rate","verdict":"over-engineered","tags":["Data Science","Machine Learning"],"date":"2025-01-05T00:00:00.000Z","summary":"AI rediscovers linear regression, claims victory over candles.","paperLink":"https://doi.org/example-ml-candle"},"content":"<h2>The Paper</h2>\n<p>Researchers trained a <strong>neural network</strong> to predict how long a candle will burn based on its dimensions and wax composition.</p>\n<h3>The Setup</h3>\n<ul>\n<li>Collected data on 500 candles</li>\n<li>Features: height, diameter, wax type, wick size</li>\n<li>Target: burn time</li>\n<li>Solution: 3-layer neural network with dropout regularization</li>\n</ul>\n<h3>The Results</h3>\n<ul>\n<li>Prediction accuracy: 94.2%</li>\n<li>Model: 47,000 parameters</li>\n<li>Training time: 6 hours on GPU</li>\n<li><strong>Alternative method</strong>: Linear regression in 0.3 seconds with 93.8% accuracy</li>\n</ul>\n<p>Wait, what? ü§î</p>\n<hr>\n<h2>Sense ‚úÖ</h2>\n<p>Let&#39;s be fair - the methodology is solid:</p>\n<h3>What They Did Right</h3>\n<p><strong>1. Proper data collection</strong></p>\n<ul>\n<li>Measured 500 candles systematically</li>\n<li>Controlled laboratory conditions</li>\n<li>Documented all variables</li>\n<li>Validated measurements</li>\n</ul>\n<p><strong>2. Professional ML practices</strong></p>\n<ul>\n<li>Train/test split</li>\n<li>Cross-validation</li>\n<li>Hyperparameter tuning</li>\n<li>Regularization to prevent overfitting</li>\n</ul>\n<p><strong>3. Reproducible research</strong></p>\n<ul>\n<li>Code published on GitHub</li>\n<li>Dataset available</li>\n<li>Clear documentation</li>\n<li>Others can verify results</li>\n</ul>\n<p><strong>4. Honest reporting</strong>\nThey <em>did</em> include the linear regression baseline. They acknowledged it worked almost as well. They didn&#39;t hide the truth.</p>\n<p>So what&#39;s the problem?</p>\n<hr>\n<h2>Nonsense ü§î</h2>\n<h3>The Elephant in the Room</h3>\n<p><strong>They built a neural network to solve a problem that is <em>literally linear</em>.</strong></p>\n<p>Candle burn time = (wax volume) / (burn rate)</p>\n<p>This is <strong>high school physics</strong>. The relationship between candle dimensions and burn time is fundamentally linear. There&#39;s no hidden complexity to discover.</p>\n<h3>The Absurdity Scale</h3>\n<p>Let&#39;s compare approaches:</p>\n<p><strong>Method 1: Common Sense</strong></p>\n<pre><code>time = height / burn_rate_per_inch\n</code></pre>\n<ul>\n<li>Accuracy: ~90%</li>\n<li>Time: 5 seconds</li>\n<li>Cost: $0</li>\n<li>Parameters: 1</li>\n</ul>\n<p><strong>Method 2: Linear Regression</strong></p>\n<pre><code class=\"language-python\">model = LinearRegression()\nmodel.fit(X, y)\n</code></pre>\n<ul>\n<li>Accuracy: 93.8%</li>\n<li>Time: 0.3 seconds</li>\n<li>Cost: $0</li>\n<li>Parameters: 5</li>\n</ul>\n<p><strong>Method 3: Neural Network</strong> (what they did)</p>\n<pre><code class=\"language-python\">model = Sequential([\n    Dense(128, activation=&#39;relu&#39;),\n    Dropout(0.3),\n    Dense(64, activation=&#39;relu&#39;),\n    Dense(1)\n])\n</code></pre>\n<ul>\n<li>Accuracy: 94.2%</li>\n<li>Time: 6 hours GPU</li>\n<li>Cost: ~$20 in compute</li>\n<li>Parameters: 47,000</li>\n</ul>\n<p><strong>They gained 0.4% accuracy at the cost of 6 hours and 47,000 parameters.</strong> üéâ</p>\n<hr>\n<h2>The Bigger Picture</h2>\n<h3>Why This Happens</h3>\n<p>This paper exists because:</p>\n<p><strong>1. &quot;Machine Learning&quot; gets grants</strong></p>\n<ul>\n<li>&quot;We&#39;ll use linear regression&quot; ‚Üí Rejected</li>\n<li>&quot;We&#39;ll use deep learning&quot; ‚Üí Funded</li>\n<li>Same problem, different buzzword</li>\n</ul>\n<p><strong>2. Publication bias</strong></p>\n<ul>\n<li>Journals want &quot;novel&quot; methods</li>\n<li>&quot;We used the appropriate simple solution&quot; doesn&#39;t get published</li>\n<li>&quot;We applied AI to X&quot; gets citations</li>\n</ul>\n<p><strong>3. Resume building</strong></p>\n<ul>\n<li>Junior researchers need ML papers</li>\n<li>Whether ML was <em>necessary</em> is irrelevant</li>\n<li>It&#39;s about demonstrating you can use the tools</li>\n</ul>\n<p><strong>4. The AI hype cycle</strong></p>\n<ul>\n<li>Everything must involve neural networks</li>\n<li>Even when a calculator would suffice</li>\n<li>Especially when a calculator would suffice</li>\n</ul>\n<hr>\n<h2>Lesson üìö</h2>\n<h3><strong>Complexity is Not Intelligence</strong></h3>\n<p>The smartest solution is often the simplest one.</p>\n<h3>What This Teaches Us</h3>\n<p><strong>1. Match the tool to the problem</strong></p>\n<ul>\n<li>Linear problem? Linear solution.</li>\n<li>Nonlinear problem? Maybe try nonlinear methods.</li>\n<li>Don&#39;t use a neural network because it&#39;s 2025</li>\n</ul>\n<p><strong>2. Occam&#39;s Razor still applies</strong></p>\n<ul>\n<li>Simpler models are easier to interpret</li>\n<li>Fewer parameters = less can go wrong</li>\n<li>47,000 parameters to predict candle burn time is absurd</li>\n</ul>\n<p><strong>3. The &quot;Because We Can&quot; fallacy</strong>\nJust because you <em>can</em> use deep learning doesn&#39;t mean you <em>should</em>.</p>\n<p>Example:</p>\n<ul>\n<li>‚ùå &quot;We trained a GAN to predict if numbers are even or odd&quot;</li>\n<li>‚úÖ &quot;We used modulo 2&quot;</li>\n</ul>\n<p><strong>4. Incremental improvements have costs</strong>\nThat 0.4% accuracy boost:</p>\n<ul>\n<li>Requires GPU</li>\n<li>Takes 6 hours</li>\n<li>Needs 47,000 parameters</li>\n<li>Can&#39;t be explained to stakeholders</li>\n<li>Breaks if you look at it wrong</li>\n</ul>\n<hr>\n<h2>Real-World Implications</h2>\n<p>This pattern appears everywhere in modern data science:</p>\n<h3>The Enterprise Version</h3>\n<p><strong>Client</strong>: &quot;We need to predict customer churn&quot;</p>\n<p><strong>Consultant A</strong>: &quot;Let&#39;s try logistic regression first&quot;\n<strong>Result</strong>: 87% accuracy, easy to explain, runs in seconds</p>\n<p><strong>Consultant B</strong>: &quot;Let&#39;s build a transformer model with attention mechanisms&quot;\n<strong>Result</strong>: 88% accuracy, impossible to explain, requires ML engineer on staff</p>\n<p><strong>Who gets hired?</strong> </p>\n<p>Consultant B. Because it&#39;s 2025 and &quot;we use AI&quot; sounds better than &quot;we use statistics.&quot;</p>\n<hr>\n<h2>The Irony</h2>\n<p>The researchers demonstrated:</p>\n<ul>\n<li>Skill with modern ML tools ‚úÖ</li>\n<li>Proper experimental design ‚úÖ</li>\n<li>Honest reporting ‚úÖ</li>\n<li><strong>Complete lack of judgment</strong> ‚ùå</li>\n</ul>\n<h3>What They Should Have Done</h3>\n<p><strong>Title</strong>: &quot;Comparing Prediction Methods for Candle Burn Time&quot;</p>\n<p><strong>Conclusion</strong>: &quot;Linear regression is sufficient. Neural networks provide no meaningful improvement. Don&#39;t use deep learning for linear problems. Thanks for coming to our TED talk.&quot;</p>\n<p><strong>Impact</strong>: Would have saved the world from thousands of similar papers.</p>\n<p><strong>What they did instead</strong>: Contribute to the pile of &quot;AI for Everything&quot; papers that make everyone think they need neural networks to predict anything.</p>\n<hr>\n<h2>The Meta-Lesson</h2>\n<h3>AI is Amazing... When Appropriate</h3>\n<p>Neural networks have revolutionized:</p>\n<ul>\n<li>Image recognition</li>\n<li>Natural language processing</li>\n<li>Game playing</li>\n<li>Drug discovery</li>\n<li>Actually complex problems</li>\n</ul>\n<p>But using them for candle burn time prediction is like:</p>\n<ul>\n<li>Using a flamethrower to light a candle</li>\n<li>Hiring a lawyer to open a door</li>\n<li>Getting an MRI for a paper cut</li>\n</ul>\n<p><strong>Technically possible. Hilariously inappropriate.</strong></p>\n<hr>\n<h2>Why This Belongs at IAN</h2>\n<p>This paper represents <strong>peak academic overengineering</strong>:</p>\n<ul>\n<li>Methodologically sound ‚úÖ</li>\n<li>Utterly unnecessary ‚úÖ</li>\n<li>Symptom of larger problem ‚úÖ</li>\n<li>Makes us question humanity&#39;s priorities ‚úÖ</li>\n</ul>\n<p>It&#39;s the research equivalent of using a sledgehammer to hang a picture frame - sure, the nail went in, but was that really the best approach?</p>\n<p>üß∞ <strong>Our verdict</strong>: Over-engineered to the point of parody.</p>\n<hr>\n<p><strong>The Real Tragedy:</strong></p>\n<p>Someone spent months on this. They could have:</p>\n<ul>\n<li>Solved an actually hard problem</li>\n<li>Discovered something new</li>\n<li>Advanced human knowledge</li>\n</ul>\n<p>Instead, they proved that neural networks can... do multiplication slowly.</p>\n<p>And that&#39;s why we&#39;re here. üê∏</p>\n<hr>\n<p><strong>Paper</strong>: <a href=\"https://doi.org/example-ml-candle\">Read the original</a><br><strong>Field</strong>: Data Science / Machine Learning<br><strong>Published</strong>: 2024<br><strong>Our Verdict</strong>: üß∞ Over-engineered (magnificently so)</p>\n"},"__N_SSG":true}